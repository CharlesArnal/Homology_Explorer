# Modified so that calc_score called only once per iteration

# -*- coding: utf-8 -*-
"""RL_call_polymake.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14jFMmLcpskc7mqXy9H7ur7KitDZxeruH

"""

"""# The rest of the code

"""



# Code from the paper "Constructions in combinatorics via neural networks and LP solvers" by A Z Wagner
# Code template
#
# This code works on tensorflow version 1.14.0 and python version 3.6.3
#
# For later versions of tensorflow there seems to be a massive overhead in the predict function for some reason, and/or it produces mysterious errors.
# If the code doesn't work, make sure you are using these versions of tf and python.
#
# He used keras version 2.3.1, not sure if this is important, but I recommend this just to be safe.

#Use this file as a template if you are able to use numba njit() for the calc_score function in your problem.
#Otherwise, if this is not an option, modify the simpler code in the *demos* folder

#import networkx as nx #for various graph parameters, such as eigenvalues, macthing number, etc. Does not work with numba (yet)
import random
import numpy as np
# Apparently unused
#from keras.utils import to_categorical

import tensorflow as tf
from tensorflow import keras
#from keras.models import Sequential
#from keras.layers import Dense
#from keras.optimizers import SGD, Adam
#from keras.models import load_model
from statistics import mean
import pickle
import time
import os
import sys
from math import comb
import matplotlib.pyplot as plt
from numba import njit

import subprocess

from score_polymake_wrapper import calc_score, jitted_calc_score

print("Using VAE to optimize signs distribution.\n")

N_LAYERS = int(sys.argv[1])
WIDTH_LAYERS = [int(width) for width in sys.argv[2:2+N_LAYERS]]
DEGREE, DIMENSION, LEARNING_RATE, \
	LOCAL_PATH, SIGNS_FILE, OUTPUT_SCORING_FILE, POLYMAKE_SCORING_SCRIPT,\
	INPUT_TRIANGULATION_MONOMIALS_FILE, INPUT_TRIANGULATION_COEFFS_FILE, \
	TEMP_HOMOLOGIES_FILE,  FIND_NEW_TOPOLOGIES, LIST_OF_HOMOLOGIES_FILE = sys.argv[2+N_LAYERS:]

DEGREE = int(DEGREE)
DIMENSION = int(DIMENSION)
LEARNING_RATE = float(LEARNING_RATE)

FIND_NEW_TOPOLOGIES = True if FIND_NEW_TOPOLOGIES == "True" else False


# Get the number of signs
with open(os.path.join(LOCAL_PATH, INPUT_TRIANGULATION_COEFFS_FILE), 'r') as f:
	triangulation_coeffs =  np.loadtxt(f,dtype=float)
	N_SIGNS = len(triangulation_coeffs)
	observation_space = 2*N_SIGNS
	print(f"\nNumber of signs to generate : {N_SIGNS}\n")



#-------------------------------------

# get a very large number of training samples (sequences and associated betti numbers) from other methods,
# see if it has any hope of succeeding (i.e. if the reconstruction score becomes reasonably small)

# Same with a deep NN (using Keras)
#from sklearn.model_selection import cross_val_score
import tensorflow as tf
from tensorflow import keras

#[batch_size,N_signs]
train = 
test = 
# [batch_size]
train_labels = 
test_labels = 

#TODO finir cette partie
input_shape=[np.shape(train)[-1]]
model = keras.models.Sequential()
model.add(keras.layers.InputLayer(input_shape=input_shape))
for layer in range(n_hidden):
    model.add(keras.layers.Dense(n_neurons, activation="relu"))
  model.add(keras.layers.Dense(1))
  optimizer = keras.optimizers.SGD(learning_rate=learning_rate)
  model.compile(loss="mse", optimizer="adam")

def build_model(n_hidden=2, n_neurons=20, learning_rate=1e-2,
input_shape=[np.shape(train)[-1]]):
  model = keras.models.Sequential()
  model.add(keras.layers.InputLayer(input_shape=input_shape))
  for layer in range(n_hidden):
    model.add(keras.layers.Dense(n_neurons, activation="relu"))
  model.add(keras.layers.Dense(1))
  optimizer = keras.optimizers.SGD(learning_rate=learning_rate)
  model.compile(loss="mse", optimizer="adam")
  return model


keras_reg = tf.keras.wrappers.scikit_learn.KerasRegressor(build_model)

#scores = -cross_val_score(keras_reg, train, train_labels, scoring="neg_mean_squared_error", cv=10)

keras_reg.fit(train, train_labels, epochs=300)

predictions_training = keras_reg.predict(train)
error_training = mean_squared_error(predictions_training , train_labels)
print("training error = "+str(error_training))
predictions = keras_reg.predict(test)
error = mean_squared_error(predictions, test_labels)
print("test error = "+str(error))

def display_scores(scores):
  print("Scores:", scores)
  print("Mean:", scores.mean())
  print("Standard deviation:", scores.std())

#display_scores(scores)
# Doesn't work at all with small number of examples
# With 2500 samples, 0.17


model = keras.Sequential()
for width in WIDTH_LAYERS:
	model.add(keras.layers.Dense(width,  activation="relu"))
model.add(keras.layers.Dense(1, activation="sigmoid"))
model.build((None, observation_space))
model.compile(loss="binary_crossentropy", optimizer=keras.optimizers.SGD(learning_rate = LEARNING_RATE)) #Adam optimizer also works well, with lower learning rate

print(model.summary())

